{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-06T16:13:38.527123Z",
     "start_time": "2025-10-06T16:13:35.002572Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "device = \"cuda\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1f01c469f40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2fded8d5d8c4a8d9",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-06T16:13:41.916671Z",
     "start_time": "2025-10-06T16:13:38.539714Z"
    }
   },
   "source": [
    "class Physionet(Dataset):\n",
    "    def __init__(self, root, train=\"train\"):\n",
    "        raw_data =np.load(os.path.join(root, train + \"_audios.npy\"), allow_pickle=True)\n",
    "        self.labels = []\n",
    "        self.datas = []\n",
    "        for item in raw_data:\n",
    "            self.datas.append(item[2])\n",
    "            self.labels.append(item[1])\n",
    "        self.datas = np.array(self.datas)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "        self.datas = torch.from_numpy(self.datas).float().to(device)\n",
    "        self.labels = torch.from_numpy(self.labels).float().to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.datas[index], self.labels[index]\n",
    "\n",
    "# data_path = \"..\\\\..\\\\..\\\\tydata\\\\BMD_audio\" # Z:\\mouse\\tydata\\BMD_audio\n",
    "data_path = \"../../tydata/BMD_audio\"\n",
    "\n",
    "train_dataset = Physionet(data_path, train=\"train\")\n",
    "val_dataset = Physionet(data_path, train=\"val\")\n",
    "test_dataset = Physionet(data_path, train=\"test\")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Val: {len(val_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 16542\n",
      "Val: 2067\n",
      "Test: 2069\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "d71cf1ba243e0e2f",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-06T16:13:47.151444Z",
     "start_time": "2025-10-06T16:13:42.230444Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "# 首先，在函数get_freq_indices(method)中，根据给定的method参数，确定了需要选择的频率索引。method参数可以是以下几种取值：\n",
    "# 'top1','top2','top4','top8','top16','top32','bot1','bot2','bot4','bot8','bot16','bot32','low1','low2','low4','low8','low16','low32'。\n",
    "# 根据不同的取值，函数会返回相应数量的频率索引列表（mapper_x和mapper_y）。\n",
    "def get_freq_indices_l(method):\n",
    "    assert method in ['top1', 'top2', 'top4', 'top8', 'top16', 'top32',\n",
    "                      'bot1', 'bot2', 'bot4', 'bot8', 'bot16', 'bot32',\n",
    "                      'low1', 'low2', 'low4', 'low8', 'low16', 'low32']\n",
    "    num_freq = int(method[3:])  # 当method为16时，num_freq=16\n",
    "    if 'top' in method:\n",
    "        all_top_indices_x = [0, 0, 2, 6, 0, 0, 0, 0, 4, 6, 6, 6, 1, 5, 6, 5, 3, 3, 5, 6, 2, 0, 5, 1, 4, 3, 6, 4, 5, 4,\n",
    "                             4, 3]\n",
    "        all_top_indices_y = [1, 6, 3, 0, 4, 2, 0, 5, 0, 6, 1, 2, 2, 0, 3, 1, 3, 0, 2, 4, 0, 3, 5, 5, 4, 6, 5, 2, 6, 1,\n",
    "                             5, 4]\n",
    "        mapper_x = all_top_indices_x[:num_freq]\n",
    "        mapper_y = all_top_indices_y[:num_freq]\n",
    "    elif 'low' in method:\n",
    "        all_low_indices_x = [0, 0, 1, 1, 0, 2, 2, 1, 2, 0, 3, 4, 0, 1, 3, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 1, 2,\n",
    "                             3, 4]\n",
    "        all_low_indices_y = [0, 1, 0, 1, 2, 0, 1, 2, 2, 3, 0, 0, 4, 3, 1, 5, 4, 3, 2, 1, 0, 6, 5, 4, 3, 2, 1, 0, 6, 5,\n",
    "                             4, 3]\n",
    "        mapper_x = all_low_indices_x[:num_freq]\n",
    "        mapper_y = all_low_indices_y[:num_freq]\n",
    "    elif 'bot' in method:\n",
    "        all_bot_indices_x = [6, 1, 3, 3, 2, 4, 1, 2, 4, 4, 5, 1, 4, 6, 2, 5, 6, 1, 6, 2, 2, 4, 3, 3, 5, 5, 6, 2, 5, 5,\n",
    "                             3, 6]\n",
    "        all_bot_indices_y = [6, 4, 4, 6, 6, 3, 1, 4, 4, 5, 6, 5, 2, 2, 5, 1, 4, 3, 5, 0, 3, 1, 1, 2, 4, 2, 1, 1, 5, 3,\n",
    "                             3, 3]\n",
    "        mapper_x = all_bot_indices_x[:num_freq]\n",
    "        mapper_y = all_bot_indices_y[:num_freq]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return mapper_x, mapper_y\n",
    "\n",
    "\n",
    "def get_freq_indices_r(method):\n",
    "    assert method in ['top1', 'top2', 'top4', 'top8', 'top16', 'top32',\n",
    "                      'bot1', 'bot2', 'bot4', 'bot8', 'bot16', 'bot32',\n",
    "                      'low1', 'low2', 'low4', 'low8', 'low16', 'low32']\n",
    "    num_freq = int(method[3:])  # 当method为16时，num_freq=16\n",
    "    if 'top' in method:\n",
    "        all_top_indices_x = [0, 2, 6, 0, 0, 0, 3, 2, 5, 2, 1, 1, 0, 0, 3, 1, 3, 4, 5, 6, 2, 3, 6, 5, 6, 1, 6, 4, 4, 4,\n",
    "                             4, 1]\n",
    "        all_top_indices_y = [3, 0, 6, 4, 1, 0, 1, 1, 1, 6, 5, 0, 6, 2, 3, 3, 0, 2, 0, 0, 2, 2, 3, 2, 1, 4, 4, 0, 4, 5,\n",
    "                             6, 6]\n",
    "        mapper_x = all_top_indices_x[:num_freq]\n",
    "        mapper_y = all_top_indices_y[:num_freq]\n",
    "    elif 'low' in method:\n",
    "        all_low_indices_x = [0, 0, 1, 1, 0, 2, 2, 1, 2, 0, 3, 4, 0, 1, 3, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 1, 2,\n",
    "                             3, 4]\n",
    "        all_low_indices_y = [0, 1, 0, 1, 2, 0, 1, 2, 2, 3, 0, 0, 4, 3, 1, 5, 4, 3, 2, 1, 0, 6, 5, 4, 3, 2, 1, 0, 6, 5,\n",
    "                             4, 3]\n",
    "        mapper_x = all_low_indices_x[:num_freq]\n",
    "        mapper_y = all_low_indices_y[:num_freq]\n",
    "    elif 'bot' in method:\n",
    "        all_bot_indices_x = [6, 1, 3, 3, 2, 4, 1, 2, 4, 4, 5, 1, 4, 6, 2, 5, 6, 1, 6, 2, 2, 4, 3, 3, 5, 5, 6, 2, 5, 5,\n",
    "                             3, 6]\n",
    "        all_bot_indices_y = [6, 4, 4, 6, 6, 3, 1, 4, 4, 5, 6, 5, 2, 2, 5, 1, 4, 3, 5, 0, 3, 1, 1, 2, 4, 2, 1, 1, 5, 3,\n",
    "                             3, 3]\n",
    "        mapper_x = all_bot_indices_x[:num_freq]\n",
    "        mapper_y = all_bot_indices_y[:num_freq]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return mapper_x, mapper_y\n",
    "\n",
    "\n",
    "# 多个频率+通道+空间注意力机制\n",
    "class MultiSpectralChannelSpatialAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, channel, dct_h, dct_w, reduction=16, freq_sel_method='top8', side='l'):\n",
    "        super(MultiSpectralChannelSpatialAttentionLayer, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.dct_h = dct_h\n",
    "        self.dct_w = dct_w\n",
    "\n",
    "        if side == 'l':\n",
    "            mapper_x, mapper_y = get_freq_indices_l(freq_sel_method)\n",
    "        else:\n",
    "            mapper_x, mapper_y = get_freq_indices_r(freq_sel_method)\n",
    "        self.num_split = len(mapper_x)\n",
    "        mapper_x = [temp_x * (dct_h // 7) for temp_x in mapper_x]\n",
    "        mapper_y = [temp_y * (dct_w // 7) for temp_y in mapper_y]\n",
    "        # make the frequencies in different sizes are identical to a 7x7 frequency space\n",
    "        # eg, (2,2) in 14x14 is identical to (1,1) in 7x7\n",
    "\n",
    "        # frequency\n",
    "        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, channel)\n",
    "        # channel\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # spatial\n",
    "        self.SpatialAttention = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        x_pooled = x\n",
    "        if h != self.dct_h or w != self.dct_w:\n",
    "            x_pooled = torch.nn.functional.adaptive_avg_pool2d(x, (self.dct_h, self.dct_w))\n",
    "            # If you have concerns about one-line-change, don't worry.   :)\n",
    "            # In the ImageNet models, this line will never be triggered. \n",
    "            # This is for compatibility in instance segmentation and object detection.\n",
    "        y = self.dct_layer(x_pooled)\n",
    "\n",
    "        y = self.fc(y).view(n, c, 1, 1)\n",
    "        y = x * y.expand_as(x)\n",
    "\n",
    "        y = self.SpatialAttention(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class MultiSpectralDCTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Generate dct filters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, height, width, mapper_x, mapper_y, channel):\n",
    "        super(MultiSpectralDCTLayer, self).__init__()\n",
    "\n",
    "        assert len(mapper_x) == len(mapper_y)\n",
    "        # print(f\"x {channel}  weight {len(mapper_x)}\")\n",
    "        # assert channel % len(mapper_x) == 0\n",
    "\n",
    "        self.num_freq = len(mapper_x)\n",
    "\n",
    "        # fixed DCT init\n",
    "        self.register_buffer('weight', self.get_dct_filter(height, width, mapper_x, mapper_y, channel))\n",
    "\n",
    "        # fixed random init\n",
    "        # self.register_buffer('weight', torch.rand(channel, height, width))\n",
    "\n",
    "        # learnable DCT init\n",
    "        # self.register_parameter('weight', self.get_dct_filter(height, width, mapper_x, mapper_y, channel))\n",
    "\n",
    "        # learnable random init\n",
    "        # self.register_parameter('weight', torch.rand(channel, height, width))\n",
    "\n",
    "        # num_freq, h, w\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 4, 'x must been 4 dimensions, but got ' + str(len(x.shape))\n",
    "        # n, c, h, w = x.shape\n",
    "        # print(f\"x :{x.shape}  weight: {self.weight.shape}\")\n",
    "        x = x * self.weight\n",
    "\n",
    "        result = torch.sum(x, dim=[2, 3])\n",
    "        return result\n",
    "\n",
    "    def build_filter(self, pos, freq, POS):\n",
    "        result = math.cos(math.pi * freq * (pos + 0.5) / POS) / math.sqrt(POS)\n",
    "        if freq == 0:\n",
    "            return result\n",
    "        else:\n",
    "            return result * math.sqrt(2)\n",
    "\n",
    "    def get_dct_filter(self, tile_size_x, tile_size_y, mapper_x, mapper_y, channel):\n",
    "        dct_filter = torch.zeros(channel, tile_size_x, tile_size_y)\n",
    "\n",
    "        c_part = channel // len(mapper_x)\n",
    "\n",
    "        for i, (u_x, v_y) in enumerate(zip(mapper_x, mapper_y)):\n",
    "            for t_x in range(tile_size_x):\n",
    "                for t_y in range(tile_size_y):\n",
    "                    dct_filter[i * c_part: (i + 1) * c_part, t_x, t_y] = self.build_filter(t_x, u_x,\n",
    "                                                                                           tile_size_x) * self.build_filter(\n",
    "                        t_y, v_y, tile_size_y)\n",
    "\n",
    "        return dct_filter\n",
    "\n",
    "\n",
    "# 通道注意力\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction_ratio),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction_ratio, in_channels)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * self.sigmoid(y)\n",
    "\n",
    "\n",
    "# 空间注意力\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_pool = torch.max(x, dim=1, keepdim=True)[0]\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "        y = torch.cat([max_pool, avg_pool], dim=1)\n",
    "        y = self.conv(y)\n",
    "        return x * self.sigmoid(y)\n",
    "\n",
    "\n",
    "# 通道+空间注意力 CBAM\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_attention(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "import librosa\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ZHJNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        global _mapper_x, _mapper_y  #ty\n",
    "        c2wh_l = [(36, 96), (18, 48), (18, 48), (9, 24), (9, 24), (9, 24)]  #ty big left\n",
    "        c2wh_r = [(36, 96), (36, 96), (18, 48), (18, 48), (18, 48), (9, 24)]  #ty small right\n",
    "        c_l = [16, 64, 128, 256, 512, 512]\n",
    "        c_r = [16, 16, 32, 64, 128, 512]\n",
    "\n",
    "        self.in_channels = 16\n",
    "\n",
    "        self.filterbank_l = torch.tensor(librosa.filters.mel(\n",
    "            sr=1000,\n",
    "            n_fft=1024,\n",
    "            n_mels=40,\n",
    "            fmin=0.0,\n",
    "            fmax=None,\n",
    "            htk=False,\n",
    "            norm='slaney',\n",
    "        ).T).to(device)\n",
    "        self.mask_l = (self.filterbank_l == 0).float().unsqueeze(0).to(device)\n",
    "        self.inv_mask_l = 1 - self.mask_l\n",
    "        self.filterbank_non_trainable_l = self.filterbank_l.clone().detach()\n",
    "        self.filterbank_non_trainable_l.requires_grad = False\n",
    "        self.filterbank_l = nn.Parameter(self.filterbank_l)\n",
    "        print(\"left\",self.filterbank_l.device, self.mask_l.device)\n",
    "\n",
    "        self.filterbank_r = torch.tensor(librosa.filters.mel(\n",
    "            sr=1000,\n",
    "            n_fft=1024,\n",
    "            n_mels=40,\n",
    "            fmin=0.0,\n",
    "            fmax=None,\n",
    "            htk=False,\n",
    "            norm='slaney',\n",
    "        ).T).to(device)\n",
    "        self.mask_r = (self.filterbank_r == 0).float().unsqueeze(0).to(device)\n",
    "        self.inv_mask_r = 1 - self.mask_r\n",
    "        self.filterbank_non_trainable_r = self.filterbank_r.clone().detach()\n",
    "        self.filterbank_non_trainable_r.requires_grad = False\n",
    "        self.filterbank_r = nn.Parameter(self.filterbank_r)\n",
    "        print(\"right\",self.filterbank_r.device, self.mask_r.device)\n",
    "\n",
    "        self.conv_l_0 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=7, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        attention_num = 0\n",
    "        self.conv_l_1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BasicBlock(16, 16, 1),\n",
    "            BasicBlock(16, 16, 1),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_l[attention_num], c2wh_l[attention_num][0],\n",
    "                                                      c2wh_l[attention_num][1], side='l')  #ty\n",
    "        )\n",
    "\n",
    "        attention_num = 1\n",
    "        self.conv_l_2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(16, 16, 2),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_l[attention_num], c2wh_l[attention_num][0],\n",
    "                                                      c2wh_l[attention_num][1], side='l')  #ty\n",
    "        )\n",
    "\n",
    "        attention_num = 2\n",
    "        self.conv_l_3 = nn.Sequential(\n",
    "            Bottleneck(64, 32, 1),\n",
    "            Bottleneck(128, 32, 1),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_l[attention_num], c2wh_l[attention_num][0],\n",
    "                                                      c2wh_l[attention_num][1], side='l')  #ty\n",
    "        )\n",
    "\n",
    "        attention_num = 3\n",
    "        self.conv_l_4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(128, 64, 2),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_l[attention_num], c2wh_l[attention_num][0],\n",
    "                                                      c2wh_l[attention_num][1], side='l')  #ty\n",
    "        )\n",
    "\n",
    "        attention_num = 4\n",
    "        self.conv_l_5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(256, 64, 1),\n",
    "            nn.Conv2d(256, 256, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(256, 128, 1),\n",
    "            Bottleneck(512, 128, 1),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_l[attention_num], c2wh_l[attention_num][0],\n",
    "                                                      c2wh_l[attention_num][1], side='l')  #ty\n",
    "        )\n",
    "\n",
    "        attention_num = 5\n",
    "        self.conv_l_6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(512, 128, 1),\n",
    "            Bottleneck(512, 128, 1),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_l[attention_num], c2wh_l[attention_num][0],\n",
    "                                                      c2wh_l[attention_num][1], side='l')  #ty\n",
    "        )\n",
    "\n",
    "        self.conv_l_12 = nn.Conv2d(16, 64, kernel_size=1)\n",
    "        self.conv_l_12_3x3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv_l_23 = nn.Conv2d(64, 128, kernel_size=1)\n",
    "        self.conv_l_23_1x1 = nn.Conv2d(128, 128, kernel_size=1)\n",
    "        self.conv_l_34 = nn.Conv2d(128, 256, kernel_size=1)\n",
    "        self.conv_l_34_3x3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv_l_45 = nn.Conv2d(256, 512, kernel_size=1)\n",
    "        self.conv_l_45_1x1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.conv_l_56_1x1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "\n",
    "        self.bn_l_12 = nn.BatchNorm2d(64)\n",
    "        self.bn_l_23 = nn.BatchNorm2d(128)\n",
    "        self.bn_l_34 = nn.BatchNorm2d(256)\n",
    "        self.bn_l_45 = nn.BatchNorm2d(512)\n",
    "        self.bn_l_56 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.conv_r_0 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=7, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        attention_num = 0\n",
    "        self.conv_r_1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BasicBlock(16, 16, 1),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_r[attention_num], c2wh_r[attention_num][0],\n",
    "                                                      c2wh_r[attention_num][1], side='r')  #ty\n",
    "        )\n",
    "\n",
    "        attention_num = 1\n",
    "        self.conv_r_2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BasicBlock(16, 16, 1),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_r[attention_num], c2wh_r[attention_num][0],\n",
    "                                                      c2wh_r[attention_num][1], side='r')  #ty\n",
    "        )\n",
    "\n",
    "        attention_num = 2\n",
    "        self.conv_r_3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BasicBlock(16, 16, 1),\n",
    "            Bottleneck(16, 8, 2),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_r[attention_num], c2wh_r[attention_num][0],\n",
    "                                                      c2wh_r[attention_num][1], side='r')  #ty\n",
    "        )\n",
    "\n",
    "        attention_num = 3\n",
    "        self.conv_r_4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BasicBlock(32, 64, 1),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_r[attention_num], c2wh_r[attention_num][0],\n",
    "                                                      c2wh_r[attention_num][1], side='r')  #ty\n",
    "        )\n",
    "\n",
    "        attention_num = 4\n",
    "        self.conv_r_5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(64, 16, 1),\n",
    "            Bottleneck(64, 16, 1),\n",
    "            Bottleneck(64, 32, 1),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_r[attention_num], c2wh_r[attention_num][0],\n",
    "                                                      c2wh_r[attention_num][1], side='r'),  #ty\n",
    "        )\n",
    "\n",
    "        attention_num = 5\n",
    "        self.conv_r_6 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(128, 128, 2),\n",
    "            MultiSpectralChannelSpatialAttentionLayer(c_r[attention_num], c2wh_r[attention_num][0],\n",
    "                                                      c2wh_r[attention_num][1], side='r')  #ty\n",
    "        )\n",
    "\n",
    "        self.conv_r_12_1x1 = nn.Conv2d(16, 16, kernel_size=1)\n",
    "        self.conv_r_23 = nn.Conv2d(16, 32, kernel_size=1)\n",
    "        self.conv_r_23_3x3 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv_r_34 = nn.Conv2d(32, 64, kernel_size=1)\n",
    "        self.conv_r_34_1x1 = nn.Conv2d(64, 64, kernel_size=1)\n",
    "        self.conv_r_45 = nn.Conv2d(64, 128, kernel_size=1)\n",
    "        self.conv_r_45_1x1 = nn.Conv2d(128, 128, kernel_size=1)\n",
    "        self.conv_r_56 = nn.Conv2d(128, 512, kernel_size=1)\n",
    "        self.conv_r_56_3x3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.bn_r_12 = nn.BatchNorm2d(16)\n",
    "        self.bn_r_23 = nn.BatchNorm2d(32)\n",
    "        self.bn_r_34 = nn.BatchNorm2d(64)\n",
    "        self.bn_r_45 = nn.BatchNorm2d(128)\n",
    "        self.bn_r_56 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.conv_m_12 = nn.Conv2d(16, 64, kernel_size=1)\n",
    "        self.conv_m_12_1x1 = nn.Conv2d(64, 64, kernel_size=1)\n",
    "        self.conv_m_23 = nn.Conv2d(32, 128, kernel_size=1)\n",
    "        self.conv_m_23_3x3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv_m_34 = nn.Conv2d(64, 256, kernel_size=1)\n",
    "        self.conv_m_34_1x1 = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        self.conv_m_45 = nn.Conv2d(128, 512, kernel_size=1)\n",
    "        self.conv_m_45_3x3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv_m_56_3x3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.bn_m_12 = nn.BatchNorm2d(64)\n",
    "        self.bn_m_23 = nn.BatchNorm2d(128)\n",
    "        self.bn_m_34 = nn.BatchNorm2d(256)\n",
    "        self.bn_m_45 = nn.BatchNorm2d(512)\n",
    "        self.bn_m_56 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.conv_f_12 = nn.Conv2d(64, 128, kernel_size=1)\n",
    "        self.conv_f_23 = nn.Conv2d(128, 256, kernel_size=1, stride=2)\n",
    "        self.conv_f_34 = nn.Conv2d(256, 512, kernel_size=1)\n",
    "        self.bn_f_123 = nn.BatchNorm2d(128)\n",
    "        self.bn_f_1234 = nn.BatchNorm2d(256)\n",
    "        self.bn_f_12345 = nn.BatchNorm2d(512)\n",
    "        self.bn_f_123456 = nn.BatchNorm2d(512)\n",
    "        self.conv_f_123_1x1 = nn.Conv2d(128, 128, kernel_size=1)\n",
    "        self.conv_f_1234_1x1 = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        self.conv_f_12345_1x1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.conv_f_123456_1x1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "\n",
    "        self.decision_layer = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 2),\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(Bottleneck(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * Bottleneck.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # left\n",
    "        waveform_to_stft = torch.stft(x, n_fft=1024, win_length=50, hop_length=25,\n",
    "                                      window=torch.hann_window(50).to(device), return_complex=True)\n",
    "        stft_to_stftm = torch.transpose(torch.abs(waveform_to_stft), 1, 2).to(device)\n",
    "        non_trainable_part = self.filterbank_non_trainable_l * self.mask_l\n",
    "        trainable_part = self.filterbank_l * self.inv_mask_l\n",
    "        stftm_to_melgram = torch.matmul(stft_to_stftm, trainable_part + non_trainable_part)\n",
    "        x_l = stftm_to_melgram[:,:100,:].unsqueeze(1)\n",
    "\n",
    "        x_l_0 = self.conv_l_0(x_l)\n",
    "        x_l_1 = self.conv_l_1(x_l_0)\n",
    "        x_l_2 = self.conv_l_2(x_l_1)\n",
    "        x_l_3 = self.conv_l_3(x_l_2)\n",
    "        x_l_4 = self.conv_l_4(x_l_3)\n",
    "        x_l_5 = self.conv_l_5(x_l_4)\n",
    "        x_l_6 = self.conv_l_6(x_l_5)\n",
    "\n",
    "        x_l_12 = F.relu(self.bn_l_12(\n",
    "            self.conv_l_12_3x3(self.conv_l_12(x_l_1) + F.interpolate(x_l_2, scale_factor=2, mode='nearest'))),\n",
    "            inplace=True)\n",
    "        x_l_23 = F.relu(self.bn_l_23(self.conv_l_23_1x1(self.conv_l_23(x_l_2) + x_l_3)), inplace=True)\n",
    "        x_l_34 = F.relu(self.bn_l_34(\n",
    "            self.conv_l_34_3x3(self.conv_l_34(x_l_3) + F.interpolate(x_l_4, scale_factor=2, mode='nearest'))),\n",
    "            inplace=True)\n",
    "        x_l_45 = F.relu(self.bn_l_45(self.conv_l_45_1x1(self.conv_l_45(x_l_4) + x_l_5)), inplace=True)\n",
    "        x_l_56 = F.relu(self.bn_l_56(self.conv_l_56_1x1(x_l_5 + x_l_6)), inplace=True)\n",
    "\n",
    "        # right\n",
    "        waveform_to_stft = torch.stft(x, n_fft=1024, win_length=50, hop_length=25,\n",
    "                                      window=torch.hann_window(50).to(device), return_complex=True)\n",
    "        stft_to_stftm = torch.transpose(torch.abs(waveform_to_stft), 1, 2).to(device)\n",
    "        non_trainable_part = self.filterbank_non_trainable_r * self.mask_r\n",
    "        trainable_part = self.filterbank_r * self.inv_mask_r\n",
    "        stftm_to_melgram = torch.matmul(stft_to_stftm, trainable_part + non_trainable_part)\n",
    "        x_r = stftm_to_melgram[:,:100,:].unsqueeze(1)\n",
    "\n",
    "        x_r_0 = self.conv_r_0(x_r)\n",
    "        x_r_1 = self.conv_r_1(x_r_0)\n",
    "        x_r_2 = self.conv_r_2(x_r_1)\n",
    "        x_r_3 = self.conv_r_3(x_r_2)\n",
    "        x_r_4 = self.conv_r_4(x_r_3)\n",
    "        x_r_5 = self.conv_r_5(x_r_4)\n",
    "        x_r_6 = self.conv_r_6(x_r_5)\n",
    "\n",
    "        x_r_12 = F.relu(self.bn_r_12(self.conv_r_12_1x1(x_r_1 + x_r_2)), inplace=True)\n",
    "        x_r_23 = F.relu(self.bn_r_23(\n",
    "            self.conv_r_23_3x3(self.conv_r_23(x_r_2) + F.interpolate(x_r_3, scale_factor=2, mode='nearest'))),\n",
    "            inplace=True)\n",
    "        x_r_34 = F.relu(self.bn_r_34(self.conv_r_34_1x1(self.conv_r_34(x_r_3) + x_r_4)), inplace=True)\n",
    "        x_r_45 = F.relu(self.bn_r_45(self.conv_r_45_1x1(self.conv_r_45(x_r_4) + x_r_5)), inplace=True)\n",
    "        x_r_56 = F.relu(self.bn_r_56(\n",
    "            self.conv_r_56_3x3(self.conv_r_56(x_r_5) + F.interpolate(x_r_6, scale_factor=2, mode='nearest'))),\n",
    "            inplace=True)\n",
    "\n",
    "        # fusion\n",
    "        x_m_12 = F.relu(self.bn_m_12(self.conv_m_12_1x1(x_l_12 + self.conv_m_12(x_r_12))), inplace=True)\n",
    "        x_m_23 = F.relu(self.bn_m_23(\n",
    "            self.conv_m_23_3x3(F.interpolate(x_l_23, scale_factor=2, mode='nearest') + self.conv_m_23(x_r_23))),\n",
    "            inplace=True)\n",
    "        x_m_34 = F.relu(self.bn_m_34(self.conv_m_34_1x1(x_l_34 + self.conv_m_34(x_r_34))), inplace=True)\n",
    "        x_m_45 = F.relu(self.bn_m_45(\n",
    "            self.conv_m_45_3x3(F.interpolate(x_l_45, scale_factor=2, mode='nearest') + self.conv_m_45(x_r_45))),\n",
    "            inplace=True)\n",
    "        x_m_56 = F.relu(\n",
    "            self.bn_m_56(self.conv_m_56_3x3(F.interpolate(x_l_56, scale_factor=2, mode='nearest') + x_r_56)),\n",
    "            inplace=True)\n",
    "\n",
    "        x_f_123 = F.relu(self.bn_f_123(self.conv_f_123_1x1(self.conv_f_12(x_m_12) + x_m_23)), inplace=True)\n",
    "        x_f_1234 = F.relu(self.bn_f_1234(self.conv_f_1234_1x1(self.conv_f_23(x_f_123) + x_m_34)), inplace=True)\n",
    "        x_f_12345 = F.relu(self.bn_f_12345(self.conv_f_12345_1x1(self.conv_f_34(x_f_1234) + x_m_45)), inplace=True)\n",
    "        x_f_123456 = F.relu(self.bn_f_123456(self.conv_f_123456_1x1((x_f_12345 + x_m_56))), inplace=True)\n",
    "        x= self.decision_layer(x_f_123456)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = ZHJNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left cuda:1 cuda:1\n",
      "right cuda:1 cuda:1\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T22:37:58.413318Z",
     "start_time": "2025-10-06T16:13:47.340640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "Min_Loss = 1000000\n",
    "MAX_UAR = 0\n",
    "logs = []\n",
    "total_train=len(train_dataset)\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in (train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    model.eval()\n",
    "    correct_train = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels_max = torch.max(labels, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels_max).sum().item()\n",
    "\n",
    "            TN += ((predicted == 1) & (labels_max == 1)).sum().item()\n",
    "            FN += ((predicted == 1) & (labels_max == 0)).sum().item()\n",
    "            TP += ((predicted == 0) & (labels_max == 0)).sum().item()\n",
    "            FP += ((predicted == 0) & (labels_max == 1)).sum().item()\n",
    "\n",
    "    Se = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    Sp = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    Pr = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    Acc = (TP + TN) / (TP + FP + TN + FN) if (TP + FP + TN + FN) > 0 else 0\n",
    "    UAR = (Se + Sp) / 2\n",
    "    F1 = (2 * Pr * Se) / (Pr + Se) if (Pr + Se) > 0 else 0\n",
    "    logs.append([epoch, train_loss / total_train, Se, Sp, Pr, F1, Acc, UAR])\n",
    "    print(f\"Epoch [{epoch}/200], \"\n",
    "          f\"Train Loss: {(train_loss / total_train):.7f}, \"\n",
    "          f\"VAL--Acc: {(correct_test / total_test) * 100:.4f}% \"\n",
    "          f\"Se:{Se * 100:.4f} \"\n",
    "          f\"Sp:{Sp * 100:.4f} \"\n",
    "          f\"Pr:{Pr * 100:.4f} \"\n",
    "          f\"F1: {F1 * 100:.4f}% \"\n",
    "          f\"Acc: {Acc * 100:.4f}% \"\n",
    "          f\"UAR: {UAR * 100:.4f}% \")\n",
    "    if (train_loss / total_train) < Min_Loss:\n",
    "        print(f\"[{epoch}] min_loss={(train_loss / total_train):.7f} \")\n",
    "        Min_Loss = (train_loss / total_train)\n",
    "    if MAX_UAR < UAR:\n",
    "        MAX_UAR = UAR\n",
    "        print(f\"[{epoch + 1}] MAX_UAR={MAX_UAR * 100:.4f}% \", end=\" \")\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "        }, f'results\\\\TwoMouses21\\\\model_{epoch}.pth')\n",
    "        print(\"saved\")\n",
    "\n",
    "print('Training Finished')\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('results/TwoMouses21/model.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(logs)"
   ],
   "id": "1addb8f25d649d22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200], Train Loss: 0.3593263, VAL--Acc: 84.6154% Se:98.4147 Sp:40.2041 Pr:84.1192 F1: 90.7072% Acc: 84.6154% UAR: 69.3094% \n",
      "[0] min_loss=0.3593263 \n",
      "[1] MAX_UAR=69.3094%  saved\n",
      "Epoch [1/200], Train Loss: 0.2747528, VAL--Acc: 86.5022% Se:98.8586 Sp:46.7347 Pr:85.6593 F1: 91.7869% Acc: 86.5022% UAR: 72.7966% \n",
      "[1] min_loss=0.2747528 \n",
      "[2] MAX_UAR=72.7966%  saved\n",
      "Epoch [2/200], Train Loss: 0.2254787, VAL--Acc: 89.8403% Se:98.2879 Sp:62.6531 Pr:89.4403 F1: 93.6556% Acc: 89.8403% UAR: 80.4705% \n",
      "[2] min_loss=0.2254787 \n",
      "[3] MAX_UAR=80.4705%  saved\n",
      "Epoch [3/200], Train Loss: 0.2072685, VAL--Acc: 91.6304% Se:96.4490 Sp:76.1224 Pr:92.8571 F1: 94.6190% Acc: 91.6304% UAR: 86.2857% \n",
      "[3] min_loss=0.2072685 \n",
      "[4] MAX_UAR=86.2857%  saved\n",
      "Epoch [4/200], Train Loss: 0.1907352, VAL--Acc: 90.9531% Se:97.5904 Sp:69.5918 Pr:91.1730 F1: 94.2726% Acc: 90.9531% UAR: 83.5911% \n",
      "[4] min_loss=0.1907352 \n",
      "Epoch [5/200], Train Loss: 0.1730224, VAL--Acc: 77.5520% Se:72.2892 Sp:94.4898 Pr:97.6864 F1: 83.0904% Acc: 77.5520% UAR: 83.3895% \n",
      "[5] min_loss=0.1730224 \n",
      "Epoch [6/200], Train Loss: 0.1677050, VAL--Acc: 91.8239% Se:96.1953 Sp:77.7551 Pr:93.2964 F1: 94.7237% Acc: 91.8239% UAR: 86.9752% \n",
      "[6] min_loss=0.1677050 \n",
      "[7] MAX_UAR=86.9752%  saved\n",
      "Epoch [7/200], Train Loss: 0.1523844, VAL--Acc: 60.9095% Se:49.2708 Sp:98.3673 Pr:98.9809 F1: 65.7917% Acc: 60.9095% UAR: 73.8191% \n",
      "[7] min_loss=0.1523844 \n",
      "Epoch [8/200], Train Loss: 0.1451074, VAL--Acc: 92.7915% Se:95.5612 Sp:83.8776 Pr:95.0189 F1: 95.2893% Acc: 92.7915% UAR: 89.7194% \n",
      "[8] min_loss=0.1451074 \n",
      "[9] MAX_UAR=89.7194%  saved\n",
      "Epoch [9/200], Train Loss: 0.1326337, VAL--Acc: 89.8403% Se:89.6005 Sp:90.6122 Pr:96.8472 F1: 93.0830% Acc: 89.8403% UAR: 90.1064% \n",
      "[9] min_loss=0.1326337 \n",
      "[10] MAX_UAR=90.1064%  saved\n",
      "Epoch [10/200], Train Loss: 0.1199347, VAL--Acc: 93.7107% Se:96.5758 Sp:84.4898 Pr:95.2470 F1: 95.9068% Acc: 93.7107% UAR: 90.5328% \n",
      "[10] min_loss=0.1199347 \n",
      "[11] MAX_UAR=90.5328%  saved\n",
      "Epoch [11/200], Train Loss: 0.1205663, VAL--Acc: 86.2603% Se:83.5130 Sp:95.1020 Pr:98.2103 F1: 90.2673% Acc: 86.2603% UAR: 89.3075% \n",
      "Epoch [12/200], Train Loss: 0.1093798, VAL--Acc: 86.2603% Se:83.7666 Sp:94.2857 Pr:97.9244 F1: 90.2939% Acc: 86.2603% UAR: 89.0262% \n",
      "[12] min_loss=0.1093798 \n",
      "Epoch [13/200], Train Loss: 0.1038301, VAL--Acc: 92.1626% Se:98.2245 Sp:72.6531 Pr:92.0380 F1: 95.0307% Acc: 92.1626% UAR: 85.4388% \n",
      "[13] min_loss=0.1038301 \n",
      "Epoch [14/200], Train Loss: 0.0960410, VAL--Acc: 89.1630% Se:87.8250 Sp:93.4694 Pr:97.7417 F1: 92.5184% Acc: 89.1630% UAR: 90.6472% \n",
      "[14] min_loss=0.0960410 \n",
      "[15] MAX_UAR=90.6472%  saved\n",
      "Epoch [15/200], Train Loss: 0.0926800, VAL--Acc: 91.5336% Se:98.1611 Sp:70.2041 Pr:91.3813 F1: 94.6500% Acc: 91.5336% UAR: 84.1826% \n",
      "[15] min_loss=0.0926800 \n",
      "Epoch [16/200], Train Loss: 0.0861998, VAL--Acc: 92.9366% Se:95.0539 Sp:86.1224 Pr:95.6605 F1: 95.3562% Acc: 92.9366% UAR: 90.5882% \n",
      "[16] min_loss=0.0861998 \n",
      "Epoch [17/200], Train Loss: 0.0821300, VAL--Acc: 92.3561% Se:98.1611 Sp:73.6735 Pr:92.3077 F1: 95.1444% Acc: 92.3561% UAR: 85.9173% \n",
      "[17] min_loss=0.0821300 \n",
      "Epoch [18/200], Train Loss: 0.0780828, VAL--Acc: 92.9850% Se:95.8782 Sp:83.6735 Pr:94.9749 F1: 95.4244% Acc: 92.9850% UAR: 89.7759% \n",
      "[18] min_loss=0.0780828 \n",
      "Epoch [19/200], Train Loss: 0.0679785, VAL--Acc: 93.0818% Se:96.5758 Sp:81.8367 Pr:94.4789 F1: 95.5158% Acc: 93.0818% UAR: 89.2063% \n",
      "[19] min_loss=0.0679785 \n",
      "Epoch [20/200], Train Loss: 0.0600155, VAL--Acc: 91.7755% Se:91.8833 Sp:91.4286 Pr:97.1831 F1: 94.4589% Acc: 91.7755% UAR: 91.6559% \n",
      "[20] min_loss=0.0600155 \n",
      "[21] MAX_UAR=91.6559%  saved\n",
      "Epoch [21/200], Train Loss: 0.0563260, VAL--Acc: 91.7755% Se:91.1858 Sp:93.6735 Pr:97.8897 F1: 94.4189% Acc: 91.7755% UAR: 92.4296% \n",
      "[21] min_loss=0.0563260 \n",
      "[22] MAX_UAR=92.4296%  saved\n",
      "Epoch [22/200], Train Loss: 0.0633708, VAL--Acc: 93.2269% Se:94.6734 Sp:88.5714 Pr:96.3848 F1: 95.5214% Acc: 93.2269% UAR: 91.6224% \n",
      "Epoch [23/200], Train Loss: 0.0546077, VAL--Acc: 91.0015% Se:90.6151 Sp:92.2449 Pr:97.4097 F1: 93.8896% Acc: 91.0015% UAR: 91.4300% \n",
      "[23] min_loss=0.0546077 \n",
      "Epoch [24/200], Train Loss: 0.0529186, VAL--Acc: 92.2109% Se:92.0736 Sp:92.6531 Pr:97.5806 F1: 94.7471% Acc: 92.2109% UAR: 92.3633% \n",
      "[24] min_loss=0.0529186 \n",
      "Epoch [25/200], Train Loss: 0.0499850, VAL--Acc: 93.1785% Se:93.4686 Sp:92.2449 Pr:97.4868 F1: 95.4354% Acc: 93.1785% UAR: 92.8568% \n",
      "[25] min_loss=0.0499850 \n",
      "[26] MAX_UAR=92.8568%  saved\n",
      "Epoch [26/200], Train Loss: 0.0452085, VAL--Acc: 91.1466% Se:90.3614 Sp:93.6735 Pr:97.8709 F1: 93.9664% Acc: 91.1466% UAR: 92.0175% \n",
      "[26] min_loss=0.0452085 \n",
      "Epoch [27/200], Train Loss: 0.0451622, VAL--Acc: 93.7591% Se:95.4344 Sp:88.3673 Pr:96.3508 F1: 95.8904% Acc: 93.7591% UAR: 91.9009% \n",
      "[27] min_loss=0.0451622 \n",
      "Epoch [28/200], Train Loss: 0.0448801, VAL--Acc: 93.7591% Se:94.3564 Sp:91.8367 Pr:97.3822 F1: 95.8454% Acc: 93.7591% UAR: 93.0966% \n",
      "[28] min_loss=0.0448801 \n",
      "[29] MAX_UAR=93.0966%  saved\n",
      "Epoch [29/200], Train Loss: 0.0441103, VAL--Acc: 93.2753% Se:93.8491 Sp:91.4286 Pr:97.2405 F1: 95.5147% Acc: 93.2753% UAR: 92.6388% \n",
      "[29] min_loss=0.0441103 \n",
      "Epoch [30/200], Train Loss: 0.0344011, VAL--Acc: 93.2269% Se:94.5466 Sp:88.9796 Pr:96.5049 F1: 95.5157% Acc: 93.2269% UAR: 91.7631% \n",
      "[30] min_loss=0.0344011 \n",
      "Epoch [31/200], Train Loss: 0.0459705, VAL--Acc: 92.9850% Se:97.9708 Sp:76.9388 Pr:93.1846 F1: 95.5178% Acc: 92.9850% UAR: 87.4548% \n",
      "Epoch [32/200], Train Loss: 0.0391607, VAL--Acc: 93.7591% Se:95.6246 Sp:87.7551 Pr:96.1735 F1: 95.8983% Acc: 93.7591% UAR: 91.6899% \n",
      "Epoch [33/200], Train Loss: 0.0305517, VAL--Acc: 91.0015% Se:98.4147 Sp:67.1429 Pr:90.6013 F1: 94.3465% Acc: 91.0015% UAR: 82.7788% \n",
      "[33] min_loss=0.0305517 \n",
      "Epoch [34/200], Train Loss: 0.0354325, VAL--Acc: 93.4204% Se:94.9905 Sp:88.3673 Pr:96.3344 F1: 95.6577% Acc: 93.4204% UAR: 91.6789% \n",
      "Epoch [35/200], Train Loss: 0.0359660, VAL--Acc: 91.3885% Se:90.6151 Sp:93.8776 Pr:97.9438 F1: 94.1370% Acc: 91.3885% UAR: 92.2463% \n",
      "Epoch [36/200], Train Loss: 0.0304603, VAL--Acc: 93.9042% Se:96.8294 Sp:84.4898 Pr:95.2589 F1: 96.0377% Acc: 93.9042% UAR: 90.6596% \n",
      "[36] min_loss=0.0304603 \n",
      "Epoch [37/200], Train Loss: 0.0299092, VAL--Acc: 89.4533% Se:98.4147 Sp:60.6122 Pr:88.9398 F1: 93.4377% Acc: 89.4533% UAR: 79.5135% \n",
      "[37] min_loss=0.0299092 \n",
      "Epoch [38/200], Train Loss: 0.0374667, VAL--Acc: 93.7107% Se:95.6880 Sp:87.3469 Pr:96.0535 F1: 95.8704% Acc: 93.7107% UAR: 91.5175% \n",
      "Epoch [39/200], Train Loss: 0.0323174, VAL--Acc: 93.6139% Se:94.6734 Sp:90.2041 Pr:96.8851 F1: 95.7665% Acc: 93.6139% UAR: 92.4388% \n",
      "Epoch [40/200], Train Loss: 0.0260075, VAL--Acc: 94.1945% Se:97.9074 Sp:82.2449 Pr:94.6658 F1: 96.2594% Acc: 94.1945% UAR: 90.0762% \n",
      "[40] min_loss=0.0260075 \n",
      "Epoch [41/200], Train Loss: 0.0334580, VAL--Acc: 93.0818% Se:96.5124 Sp:82.0408 Pr:94.5342 F1: 95.5130% Acc: 93.0818% UAR: 89.2766% \n",
      "Epoch [42/200], Train Loss: 0.0297282, VAL--Acc: 93.3237% Se:95.2441 Sp:87.1429 Pr:95.9744 F1: 95.6079% Acc: 93.3237% UAR: 91.1935% \n",
      "Epoch [43/200], Train Loss: 0.0285988, VAL--Acc: 94.0010% Se:95.5612 Sp:88.9796 Pr:96.5407 F1: 96.0484% Acc: 94.0010% UAR: 92.2704% \n",
      "Epoch [44/200], Train Loss: 0.0269931, VAL--Acc: 93.9526% Se:95.8148 Sp:87.9592 Pr:96.2420 F1: 96.0280% Acc: 93.9526% UAR: 91.8870% \n",
      "Epoch [45/200], Train Loss: 0.0269231, VAL--Acc: 94.0010% Se:96.0685 Sp:87.3469 Pr:96.0685 F1: 96.0685% Acc: 94.0010% UAR: 91.7077% \n",
      "Epoch [46/200], Train Loss: 0.0262653, VAL--Acc: 93.8075% Se:98.4781 Sp:78.7755 Pr:93.7236 F1: 96.0421% Acc: 93.8075% UAR: 88.6268% \n",
      "Epoch [47/200], Train Loss: 0.0236377, VAL--Acc: 93.3237% Se:95.1173 Sp:87.5510 Pr:96.0922 F1: 95.6023% Acc: 93.3237% UAR: 91.3342% \n",
      "[47] min_loss=0.0236377 \n",
      "Epoch [48/200], Train Loss: 0.0267708, VAL--Acc: 94.0977% Se:97.4635 Sp:83.2653 Pr:94.9351 F1: 96.1827% Acc: 94.0977% UAR: 90.3644% \n",
      "Epoch [49/200], Train Loss: 0.0252865, VAL--Acc: 93.0334% Se:94.3564 Sp:88.7755 Pr:96.4355 F1: 95.3846% Acc: 93.0334% UAR: 91.5659% \n",
      "Epoch [50/200], Train Loss: 0.0250447, VAL--Acc: 93.2269% Se:94.2930 Sp:89.7959 Pr:96.7469 F1: 95.5042% Acc: 93.2269% UAR: 92.0444% \n",
      "Epoch [51/200], Train Loss: 0.0190753, VAL--Acc: 92.8399% Se:94.9905 Sp:85.9184 Pr:95.5967 F1: 95.2926% Acc: 92.8399% UAR: 90.4544% \n",
      "[51] min_loss=0.0190753 \n",
      "Epoch [52/200], Train Loss: 0.0222955, VAL--Acc: 94.0493% Se:95.3710 Sp:89.7959 Pr:96.7825 F1: 96.0715% Acc: 94.0493% UAR: 92.5834% \n",
      "Epoch [53/200], Train Loss: 0.0245665, VAL--Acc: 94.4364% Se:97.0197 Sp:86.1224 Pr:95.7447 F1: 96.3780% Acc: 94.4364% UAR: 91.5711% \n",
      "Epoch [54/200], Train Loss: 0.0235429, VAL--Acc: 94.3880% Se:96.1953 Sp:88.5714 Pr:96.4399 F1: 96.3175% Acc: 94.3880% UAR: 92.3834% \n",
      "Epoch [55/200], Train Loss: 0.0304450, VAL--Acc: 94.4364% Se:96.1953 Sp:88.7755 Pr:96.5013 F1: 96.3480% Acc: 94.4364% UAR: 92.4854% \n",
      "Epoch [56/200], Train Loss: 0.0178897, VAL--Acc: 93.2753% Se:95.8782 Sp:84.8980 Pr:95.3342 F1: 95.6054% Acc: 93.2753% UAR: 90.3881% \n",
      "[56] min_loss=0.0178897 \n",
      "Epoch [57/200], Train Loss: 0.0238767, VAL--Acc: 93.7591% Se:97.0831 Sp:83.0612 Pr:94.8575 F1: 95.9574% Acc: 93.7591% UAR: 90.0721% \n",
      "Epoch [58/200], Train Loss: 0.0175840, VAL--Acc: 92.4045% Se:92.9613 Sp:90.6122 Pr:96.9577 F1: 94.9174% Acc: 92.4045% UAR: 91.7868% \n",
      "[58] min_loss=0.0175840 \n",
      "Epoch [59/200], Train Loss: 0.0264707, VAL--Acc: 94.7267% Se:96.4490 Sp:89.1837 Pr:96.6328 F1: 96.5408% Acc: 94.7267% UAR: 92.8163% \n",
      "Epoch [60/200], Train Loss: 0.0216615, VAL--Acc: 91.2433% Se:99.1756 Sp:65.7143 Pr:90.3002 F1: 94.5301% Acc: 91.2433% UAR: 82.4450% \n",
      "Epoch [61/200], Train Loss: 0.0157030, VAL--Acc: 93.8558% Se:97.9074 Sp:80.8163 Pr:94.2613 F1: 96.0498% Acc: 93.8558% UAR: 89.3619% \n",
      "[61] min_loss=0.0157030 \n",
      "Epoch [62/200], Train Loss: 0.0258128, VAL--Acc: 94.2912% Se:95.4978 Sp:90.4082 Pr:96.9736 F1: 96.2300% Acc: 94.2912% UAR: 92.9530% \n",
      "Epoch [63/200], Train Loss: 0.0152305, VAL--Acc: 94.5815% Se:97.2099 Sp:86.1224 Pr:95.7527 F1: 96.4758% Acc: 94.5815% UAR: 91.6662% \n",
      "[63] min_loss=0.0152305 \n",
      "Epoch [64/200], Train Loss: 0.0229369, VAL--Acc: 93.4204% Se:96.5124 Sp:83.4694 Pr:94.9470 F1: 95.7233% Acc: 93.4204% UAR: 89.9909% \n",
      "Epoch [65/200], Train Loss: 0.0187102, VAL--Acc: 94.2912% Se:96.3855 Sp:87.5510 Pr:96.1417 F1: 96.2635% Acc: 94.2912% UAR: 91.9683% \n",
      "Epoch [66/200], Train Loss: 0.0225045, VAL--Acc: 94.1461% Se:95.4978 Sp:89.7959 Pr:96.7866 F1: 96.1379% Acc: 94.1461% UAR: 92.6468% \n",
      "Epoch [67/200], Train Loss: 0.0118825, VAL--Acc: 92.9850% Se:93.8491 Sp:90.2041 Pr:96.8586 F1: 95.3301% Acc: 92.9850% UAR: 92.0266% \n",
      "[67] min_loss=0.0118825 \n",
      "Epoch [68/200], Train Loss: 0.0260705, VAL--Acc: 93.8558% Se:96.8294 Sp:84.2857 Pr:95.1995 F1: 96.0075% Acc: 93.8558% UAR: 90.5576% \n",
      "Epoch [69/200], Train Loss: 0.0186351, VAL--Acc: 92.7431% Se:99.0488 Sp:72.4490 Pr:92.0448 F1: 95.4184% Acc: 92.7431% UAR: 85.7489% \n",
      "Epoch [70/200], Train Loss: 0.0176631, VAL--Acc: 94.3396% Se:97.6538 Sp:83.6735 Pr:95.0617 F1: 96.3403% Acc: 94.3396% UAR: 90.6636% \n",
      "Epoch [71/200], Train Loss: 0.0187240, VAL--Acc: 94.0493% Se:96.4490 Sp:86.3265 Pr:95.7809 F1: 96.1137% Acc: 94.0493% UAR: 91.3877% \n",
      "Epoch [72/200], Train Loss: 0.0217731, VAL--Acc: 94.3396% Se:96.8928 Sp:86.1224 Pr:95.7393 F1: 96.3126% Acc: 94.3396% UAR: 91.5076% \n",
      "Epoch [73/200], Train Loss: 0.0173550, VAL--Acc: 93.0818% Se:96.1319 Sp:83.2653 Pr:94.8686 F1: 95.4961% Acc: 93.0818% UAR: 89.6986% \n",
      "Epoch [74/200], Train Loss: 0.0178380, VAL--Acc: 93.8075% Se:98.0342 Sp:80.2041 Pr:94.0962 F1: 96.0248% Acc: 93.8075% UAR: 89.1192% \n",
      "Epoch [75/200], Train Loss: 0.0104026, VAL--Acc: 94.0493% Se:94.6100 Sp:92.2449 Pr:97.5163 F1: 96.0412% Acc: 94.0493% UAR: 93.4275% \n",
      "[75] min_loss=0.0104026 \n",
      "[76] MAX_UAR=93.4275%  saved\n",
      "Epoch [76/200], Train Loss: 0.0144175, VAL--Acc: 93.5172% Se:95.3075 Sp:87.7551 Pr:96.1612 F1: 95.7325% Acc: 93.5172% UAR: 91.5313% \n",
      "Epoch [77/200], Train Loss: 0.0164100, VAL--Acc: 93.6139% Se:98.2879 Sp:78.5714 Pr:93.6556 F1: 95.9158% Acc: 93.6139% UAR: 88.4297% \n",
      "Epoch [78/200], Train Loss: 0.0245962, VAL--Acc: 93.6623% Se:94.9905 Sp:89.3878 Pr:96.6452 F1: 95.8107% Acc: 93.6623% UAR: 92.1891% \n",
      "Epoch [79/200], Train Loss: 0.0172958, VAL--Acc: 94.9202% Se:96.8928 Sp:88.5714 Pr:96.4646 F1: 96.6783% Acc: 94.9202% UAR: 92.7321% \n",
      "Epoch [80/200], Train Loss: 0.0134691, VAL--Acc: 94.0977% Se:95.1173 Sp:90.8163 Pr:97.0874 F1: 96.0922% Acc: 94.0977% UAR: 92.9668% \n",
      "Epoch [81/200], Train Loss: 0.0123074, VAL--Acc: 94.0493% Se:95.3075 Sp:90.0000 Pr:96.8428 F1: 96.0690% Acc: 94.0493% UAR: 92.6538% \n",
      "Epoch [82/200], Train Loss: 0.0115901, VAL--Acc: 94.2912% Se:97.4635 Sp:84.0816 Pr:95.1703 F1: 96.3033% Acc: 94.2912% UAR: 90.7726% \n",
      "Epoch [83/200], Train Loss: 0.0213292, VAL--Acc: 93.7591% Se:95.0539 Sp:89.5918 Pr:96.7097 F1: 95.8746% Acc: 93.7591% UAR: 92.3229% \n",
      "Epoch [84/200], Train Loss: 0.0121712, VAL--Acc: 93.6139% Se:96.7660 Sp:83.4694 Pr:94.9596 F1: 95.8543% Acc: 93.6139% UAR: 90.1177% \n",
      "Epoch [85/200], Train Loss: 0.0212837, VAL--Acc: 94.7267% Se:97.7172 Sp:85.1020 Pr:95.4771 F1: 96.5841% Acc: 94.7267% UAR: 91.4096% \n",
      "Epoch [86/200], Train Loss: 0.0184599, VAL--Acc: 94.6299% Se:96.5124 Sp:88.5714 Pr:96.4512 F1: 96.4818% Acc: 94.6299% UAR: 92.5419% \n",
      "Epoch [87/200], Train Loss: 0.0144367, VAL--Acc: 95.0169% Se:96.3855 Sp:90.6122 Pr:97.0626 F1: 96.7229% Acc: 95.0169% UAR: 93.4989% \n",
      "[88] MAX_UAR=93.4989%  saved\n",
      "Epoch [88/200], Train Loss: 0.0159587, VAL--Acc: 94.1461% Se:95.2441 Sp:90.6122 Pr:97.0284 F1: 96.1280% Acc: 94.1461% UAR: 92.9282% \n",
      "Epoch [89/200], Train Loss: 0.0140535, VAL--Acc: 94.6299% Se:98.0977 Sp:83.4694 Pr:95.0246 F1: 96.5367% Acc: 94.6299% UAR: 90.7835% \n",
      "Epoch [90/200], Train Loss: 0.0102540, VAL--Acc: 94.6783% Se:96.7026 Sp:88.1633 Pr:96.3361 F1: 96.5190% Acc: 94.6783% UAR: 92.4329% \n",
      "[90] min_loss=0.0102540 \n",
      "Epoch [91/200], Train Loss: 0.0179159, VAL--Acc: 94.3396% Se:95.0539 Sp:92.0408 Pr:97.4642 F1: 96.2440% Acc: 94.3396% UAR: 93.5474% \n",
      "[92] MAX_UAR=93.5474%  saved\n",
      "Epoch [92/200], Train Loss: 0.0113099, VAL--Acc: 94.0010% Se:97.9708 Sp:81.2245 Pr:94.3800 F1: 96.1419% Acc: 94.0010% UAR: 89.5977% \n",
      "Epoch [93/200], Train Loss: 0.0124057, VAL--Acc: 94.8718% Se:97.4001 Sp:86.7347 Pr:95.9400 F1: 96.6646% Acc: 94.8718% UAR: 92.0674% \n",
      "Epoch [94/200], Train Loss: 0.0134374, VAL--Acc: 94.2429% Se:96.5758 Sp:86.7347 Pr:95.9068 F1: 96.2401% Acc: 94.2429% UAR: 91.6552% \n",
      "Epoch [95/200], Train Loss: 0.0195519, VAL--Acc: 94.0010% Se:98.7318 Sp:78.7755 Pr:93.7387 F1: 96.1705% Acc: 94.0010% UAR: 88.7536% \n",
      "Epoch [96/200], Train Loss: 0.0128710, VAL--Acc: 91.5336% Se:95.6880 Sp:78.1633 Pr:93.3787 F1: 94.5193% Acc: 91.5336% UAR: 86.9256% \n",
      "Epoch [97/200], Train Loss: 0.0189280, VAL--Acc: 93.2269% Se:98.3513 Sp:76.7347 Pr:93.1532 F1: 95.6817% Acc: 93.2269% UAR: 87.5430% \n",
      "Epoch [98/200], Train Loss: 0.0138597, VAL--Acc: 94.3880% Se:96.8294 Sp:86.5306 Pr:95.8569 F1: 96.3407% Acc: 94.3880% UAR: 91.6800% \n",
      "Epoch [99/200], Train Loss: 0.0041616, VAL--Acc: 94.3396% Se:96.0685 Sp:88.7755 Pr:96.4968 F1: 96.2822% Acc: 94.3396% UAR: 92.4220% \n",
      "[99] min_loss=0.0041616 \n",
      "Epoch [100/200], Train Loss: 0.0214156, VAL--Acc: 94.1945% Se:96.8294 Sp:85.7143 Pr:95.6168 F1: 96.2193% Acc: 94.1945% UAR: 91.2719% \n",
      "Epoch [101/200], Train Loss: 0.0129579, VAL--Acc: 92.0658% Se:91.1224 Sp:95.1020 Pr:98.3573 F1: 94.6017% Acc: 92.0658% UAR: 93.1122% \n",
      "Epoch [102/200], Train Loss: 0.0142979, VAL--Acc: 94.1945% Se:96.8294 Sp:85.7143 Pr:95.6168 F1: 96.2193% Acc: 94.1945% UAR: 91.2719% \n",
      "Epoch [103/200], Train Loss: 0.0140074, VAL--Acc: 94.5331% Se:97.7806 Sp:84.0816 Pr:95.1852 F1: 96.4654% Acc: 94.5331% UAR: 90.9311% \n",
      "Epoch [104/200], Train Loss: 0.0057609, VAL--Acc: 94.5815% Se:98.3513 Sp:82.4490 Pr:94.7465 F1: 96.5152% Acc: 94.5815% UAR: 90.4001% \n",
      "Epoch [105/200], Train Loss: 0.0238686, VAL--Acc: 93.9526% Se:97.9074 Sp:81.2245 Pr:94.3765 F1: 96.1096% Acc: 93.9526% UAR: 89.5660% \n",
      "Epoch [106/200], Train Loss: 0.0067929, VAL--Acc: 94.4848% Se:96.5124 Sp:87.9592 Pr:96.2682 F1: 96.3901% Acc: 94.4848% UAR: 92.2358% \n",
      "Epoch [107/200], Train Loss: 0.0088765, VAL--Acc: 93.9042% Se:97.3367 Sp:82.8571 Pr:94.8116 F1: 96.0576% Acc: 93.9042% UAR: 90.0969% \n",
      "Epoch [108/200], Train Loss: 0.0257102, VAL--Acc: 94.4364% Se:96.3221 Sp:88.3673 Pr:96.3832 F1: 96.3527% Acc: 94.4364% UAR: 92.3447% \n",
      "Epoch [109/200], Train Loss: 0.0090899, VAL--Acc: 94.6299% Se:96.6392 Sp:88.1633 Pr:96.3338 F1: 96.4862% Acc: 94.6299% UAR: 92.4012% \n",
      "Epoch [110/200], Train Loss: 0.0052454, VAL--Acc: 93.7591% Se:97.1465 Sp:82.8571 Pr:94.8020 F1: 95.9599% Acc: 93.7591% UAR: 90.0018% \n",
      "Epoch [111/200], Train Loss: 0.0144745, VAL--Acc: 94.2912% Se:97.1465 Sp:85.1020 Pr:95.4517 F1: 96.2916% Acc: 94.2912% UAR: 91.1243% \n",
      "Epoch [112/200], Train Loss: 0.0149908, VAL--Acc: 94.9686% Se:97.0197 Sp:88.3673 Pr:96.4083 F1: 96.7130% Acc: 94.9686% UAR: 92.6935% \n",
      "Epoch [113/200], Train Loss: 0.0061337, VAL--Acc: 94.5815% Se:95.9417 Sp:90.2041 Pr:96.9250 F1: 96.4308% Acc: 94.5815% UAR: 93.0729% \n",
      "Epoch [114/200], Train Loss: 0.0121119, VAL--Acc: 93.3720% Se:93.4686 Sp:93.0612 Pr:97.7454 F1: 95.5592% Acc: 93.3720% UAR: 93.2649% \n",
      "Epoch [115/200], Train Loss: 0.0086562, VAL--Acc: 94.9686% Se:96.8294 Sp:88.9796 Pr:96.5844 F1: 96.7068% Acc: 94.9686% UAR: 92.9045% \n",
      "Epoch [116/200], Train Loss: 0.0150020, VAL--Acc: 93.4688% Se:95.8782 Sp:85.7143 Pr:95.5752 F1: 95.7265% Acc: 93.4688% UAR: 90.7963% \n",
      "Epoch [117/200], Train Loss: 0.0165013, VAL--Acc: 94.3880% Se:96.1319 Sp:88.7755 Pr:96.4990 F1: 96.3151% Acc: 94.3880% UAR: 92.4537% \n",
      "Epoch [118/200], Train Loss: 0.0042594, VAL--Acc: 94.6783% Se:98.0342 Sp:83.8776 Pr:95.1385 F1: 96.5646% Acc: 94.6783% UAR: 90.9559% \n",
      "Epoch [119/200], Train Loss: 0.0108002, VAL--Acc: 94.6299% Se:97.8440 Sp:84.2857 Pr:95.2469 F1: 96.5280% Acc: 94.6299% UAR: 91.0649% \n",
      "Epoch [120/200], Train Loss: 0.0123859, VAL--Acc: 91.5336% Se:90.6151 Sp:94.4898 Pr:98.1456 F1: 94.2301% Acc: 91.5336% UAR: 92.5524% \n",
      "Epoch [121/200], Train Loss: 0.0133883, VAL--Acc: 94.4848% Se:96.1953 Sp:88.9796 Pr:96.5627 F1: 96.3787% Acc: 94.4848% UAR: 92.5874% \n",
      "Epoch [122/200], Train Loss: 0.0180744, VAL--Acc: 94.1945% Se:96.3855 Sp:87.1429 Pr:96.0202 F1: 96.2025% Acc: 94.1945% UAR: 91.7642% \n",
      "Epoch [123/200], Train Loss: 0.0068351, VAL--Acc: 94.5815% Se:96.8294 Sp:87.3469 Pr:96.0982 F1: 96.4624% Acc: 94.5815% UAR: 92.0882% \n",
      "Epoch [124/200], Train Loss: 0.0095244, VAL--Acc: 93.3720% Se:94.4198 Sp:90.0000 Pr:96.8140 F1: 95.6019% Acc: 93.3720% UAR: 92.2099% \n",
      "Epoch [125/200], Train Loss: 0.0144300, VAL--Acc: 93.8075% Se:97.8440 Sp:80.8163 Pr:94.2578 F1: 96.0174% Acc: 93.8075% UAR: 89.3302% \n",
      "Epoch [126/200], Train Loss: 0.0078998, VAL--Acc: 93.5656% Se:97.9074 Sp:79.5918 Pr:93.9173 F1: 95.8708% Acc: 93.5656% UAR: 88.7496% \n",
      "Epoch [127/200], Train Loss: 0.0116602, VAL--Acc: 92.8399% Se:96.8928 Sp:79.7959 Pr:93.9152 F1: 95.3808% Acc: 92.8399% UAR: 88.3444% \n",
      "Epoch [128/200], Train Loss: 0.0136461, VAL--Acc: 94.5331% Se:97.8440 Sp:83.8776 Pr:95.1295 F1: 96.4676% Acc: 94.5331% UAR: 90.8608% \n",
      "Epoch [129/200], Train Loss: 0.0041643, VAL--Acc: 93.2269% Se:94.4198 Sp:89.3878 Pr:96.6256 F1: 95.5099% Acc: 93.2269% UAR: 91.9038% \n",
      "Epoch [130/200], Train Loss: 0.0064524, VAL--Acc: 93.8075% Se:98.5415 Sp:78.5714 Pr:93.6709 F1: 96.0445% Acc: 93.8075% UAR: 88.5565% \n",
      "Epoch [131/200], Train Loss: 0.0151743, VAL--Acc: 94.7267% Se:96.8928 Sp:87.7551 Pr:96.2217 F1: 96.5561% Acc: 94.7267% UAR: 92.3240% \n",
      "Epoch [132/200], Train Loss: 0.0098299, VAL--Acc: 92.5980% Se:97.5269 Sp:76.7347 Pr:93.0993 F1: 95.2617% Acc: 92.5980% UAR: 87.1308% \n",
      "Epoch [133/200], Train Loss: 0.0147641, VAL--Acc: 94.8718% Se:97.9708 Sp:84.8980 Pr:95.4293 F1: 96.6834% Acc: 94.8718% UAR: 91.4344% \n",
      "Epoch [134/200], Train Loss: 0.0064850, VAL--Acc: 94.5815% Se:96.3855 Sp:88.7755 Pr:96.5079 F1: 96.4467% Acc: 94.5815% UAR: 92.5805% \n",
      "Epoch [135/200], Train Loss: 0.0122754, VAL--Acc: 92.3561% Se:92.8345 Sp:90.8163 Pr:97.0179 F1: 94.8801% Acc: 92.3561% UAR: 91.8254% \n",
      "Epoch [136/200], Train Loss: 0.0081226, VAL--Acc: 94.4364% Se:96.5124 Sp:87.7551 Pr:96.2073 F1: 96.3596% Acc: 94.4364% UAR: 92.1337% \n",
      "Epoch [137/200], Train Loss: 0.0105794, VAL--Acc: 94.5331% Se:96.7660 Sp:87.3469 Pr:96.0957 F1: 96.4297% Acc: 94.5331% UAR: 92.0565% \n",
      "Epoch [138/200], Train Loss: 0.0030112, VAL--Acc: 94.8718% Se:96.9562 Sp:88.1633 Pr:96.3453 F1: 96.6498% Acc: 94.8718% UAR: 92.5598% \n",
      "[138] min_loss=0.0030112 \n",
      "Epoch [139/200], Train Loss: 0.0137247, VAL--Acc: 94.3880% Se:96.1953 Sp:88.5714 Pr:96.4399 F1: 96.3175% Acc: 94.3880% UAR: 92.3834% \n",
      "Epoch [140/200], Train Loss: 0.0076932, VAL--Acc: 94.8718% Se:96.0685 Sp:91.0204 Pr:97.1777 F1: 96.6199% Acc: 94.8718% UAR: 93.5444% \n",
      "Epoch [141/200], Train Loss: 0.0191189, VAL--Acc: 94.5331% Se:96.8928 Sp:86.9388 Pr:95.9799 F1: 96.4342% Acc: 94.5331% UAR: 91.9158% \n",
      "Epoch [142/200], Train Loss: 0.0072459, VAL--Acc: 95.2104% Se:96.9562 Sp:89.5918 Pr:96.7722 F1: 96.8641% Acc: 95.2104% UAR: 93.2740% \n",
      "Epoch [143/200], Train Loss: 0.0042338, VAL--Acc: 93.7107% Se:95.1807 Sp:88.9796 Pr:96.5273 F1: 95.8493% Acc: 93.7107% UAR: 92.0802% \n",
      "Epoch [144/200], Train Loss: 0.0129739, VAL--Acc: 94.0977% Se:97.2733 Sp:83.8776 Pr:95.1023 F1: 96.1755% Acc: 94.0977% UAR: 90.5754% \n",
      "Epoch [145/200], Train Loss: 0.0124304, VAL--Acc: 92.8882% Se:98.2245 Sp:75.7143 Pr:92.8657 F1: 95.4700% Acc: 92.8882% UAR: 86.9694% \n",
      "Epoch [146/200], Train Loss: 0.0058229, VAL--Acc: 94.4364% Se:96.0685 Sp:89.1837 Pr:96.6199 F1: 96.3434% Acc: 94.4364% UAR: 92.6261% \n",
      "Epoch [147/200], Train Loss: 0.0099018, VAL--Acc: 94.1461% Se:96.3855 Sp:86.9388 Pr:95.9596 F1: 96.1721% Acc: 94.1461% UAR: 91.6622% \n",
      "Epoch [148/200], Train Loss: 0.0131342, VAL--Acc: 94.1945% Se:97.9708 Sp:82.0408 Pr:94.6111 F1: 96.2617% Acc: 94.1945% UAR: 90.0058% \n",
      "Epoch [149/200], Train Loss: 0.0049949, VAL--Acc: 95.2588% Se:97.6538 Sp:87.5510 Pr:96.1899 F1: 96.9163% Acc: 95.2588% UAR: 92.6024% \n",
      "Epoch [150/200], Train Loss: 0.0007131, VAL--Acc: 95.1621% Se:96.6392 Sp:90.4082 Pr:97.0083 F1: 96.8234% Acc: 95.1621% UAR: 93.5237% \n",
      "[150] min_loss=0.0007131 \n",
      "Epoch [151/200], Train Loss: 0.0004354, VAL--Acc: 95.3072% Se:97.2099 Sp:89.1837 Pr:96.6583 F1: 96.9333% Acc: 95.3072% UAR: 93.1968% \n",
      "[151] min_loss=0.0004354 \n",
      "Epoch [152/200], Train Loss: 0.0003428, VAL--Acc: 95.4040% Se:97.5904 Sp:88.3673 Pr:96.4286 F1: 97.0060% Acc: 95.4040% UAR: 92.9789% \n",
      "[152] min_loss=0.0003428 \n",
      "Epoch [153/200], Train Loss: 0.0243611, VAL--Acc: 89.1147% Se:99.1122 Sp:56.9388 Pr:88.1060 F1: 93.2856% Acc: 89.1147% UAR: 78.0255% \n",
      "Epoch [154/200], Train Loss: 0.0189981, VAL--Acc: 94.3880% Se:96.9562 Sp:86.1224 Pr:95.7420 F1: 96.3453% Acc: 94.3880% UAR: 91.5393% \n",
      "Epoch [155/200], Train Loss: 0.0048866, VAL--Acc: 95.2104% Se:96.2587 Sp:91.8367 Pr:97.4326 F1: 96.8421% Acc: 95.2104% UAR: 94.0477% \n",
      "[156] MAX_UAR=94.0477%  saved\n",
      "Epoch [156/200], Train Loss: 0.0021133, VAL--Acc: 95.3072% Se:97.9074 Sp:86.9388 Pr:96.0199 F1: 96.9545% Acc: 95.3072% UAR: 92.4231% \n",
      "Epoch [157/200], Train Loss: 0.0072289, VAL--Acc: 93.4688% Se:96.8928 Sp:82.4490 Pr:94.6716 F1: 95.7694% Acc: 93.4688% UAR: 89.6709% \n",
      "Epoch [158/200], Train Loss: 0.0141767, VAL--Acc: 93.9526% Se:98.1611 Sp:80.4082 Pr:94.1606 F1: 96.1192% Acc: 93.9526% UAR: 89.2846% \n",
      "Epoch [159/200], Train Loss: 0.0119615, VAL--Acc: 94.4848% Se:95.4344 Sp:91.4286 Pr:97.2851 F1: 96.3508% Acc: 94.4848% UAR: 93.4315% \n",
      "Epoch [160/200], Train Loss: 0.0022079, VAL--Acc: 95.4523% Se:98.1611 Sp:86.7347 Pr:95.9702 F1: 97.0533% Acc: 95.4523% UAR: 92.4479% \n",
      "Epoch [161/200], Train Loss: 0.0010982, VAL--Acc: 94.7750% Se:96.7026 Sp:88.5714 Pr:96.4579 F1: 96.5801% Acc: 94.7750% UAR: 92.6370% \n",
      "Epoch [162/200], Train Loss: 0.0273898, VAL--Acc: 94.7267% Se:97.8440 Sp:84.6939 Pr:95.3646 F1: 96.5884% Acc: 94.7267% UAR: 91.2689% \n",
      "Epoch [163/200], Train Loss: 0.0042263, VAL--Acc: 95.0169% Se:97.4635 Sp:87.1429 Pr:96.0625 F1: 96.7579% Acc: 95.0169% UAR: 92.3032% \n",
      "Epoch [164/200], Train Loss: 0.0017326, VAL--Acc: 94.6783% Se:96.0051 Sp:90.4082 Pr:96.9891 F1: 96.4946% Acc: 94.6783% UAR: 93.2066% \n",
      "Epoch [165/200], Train Loss: 0.0006627, VAL--Acc: 94.3880% Se:95.1807 Sp:91.8367 Pr:97.4043 F1: 96.2797% Acc: 94.3880% UAR: 93.5087% \n",
      "Epoch [166/200], Train Loss: 0.0086379, VAL--Acc: 94.2429% Se:96.7026 Sp:86.3265 Pr:95.7915 F1: 96.2449% Acc: 94.2429% UAR: 91.5146% \n",
      "Epoch [167/200], Train Loss: 0.0185921, VAL--Acc: 92.8882% Se:97.0831 Sp:79.3878 Pr:93.8113 F1: 95.4191% Acc: 92.8882% UAR: 88.2354% \n",
      "Epoch [168/200], Train Loss: 0.0126552, VAL--Acc: 94.9202% Se:96.3221 Sp:90.4082 Pr:96.9987 F1: 96.6592% Acc: 94.9202% UAR: 93.3651% \n",
      "Epoch [169/200], Train Loss: 0.0029974, VAL--Acc: 94.8234% Se:97.0831 Sp:87.5510 Pr:96.1683 F1: 96.6235% Acc: 94.8234% UAR: 92.3170% \n",
      "Epoch [170/200], Train Loss: 0.0007803, VAL--Acc: 94.8718% Se:96.7026 Sp:88.9796 Pr:96.5801 F1: 96.6413% Acc: 94.8718% UAR: 92.8411% \n",
      "Epoch [171/200], Train Loss: 0.0070869, VAL--Acc: 89.8403% Se:99.1756 Sp:59.7959 Pr:88.8132 F1: 93.7088% Acc: 89.8403% UAR: 79.4858% \n",
      "Epoch [172/200], Train Loss: 0.0169550, VAL--Acc: 94.6783% Se:96.7026 Sp:88.1633 Pr:96.3361 F1: 96.5190% Acc: 94.6783% UAR: 92.4329% \n",
      "Epoch [173/200], Train Loss: 0.0083820, VAL--Acc: 94.0977% Se:97.3367 Sp:83.6735 Pr:95.0464 F1: 96.1779% Acc: 94.0977% UAR: 90.5051% \n",
      "Epoch [174/200], Train Loss: 0.0090706, VAL--Acc: 94.3880% Se:96.5124 Sp:87.5510 Pr:96.1466 F1: 96.3291% Acc: 94.3880% UAR: 92.0317% \n",
      "Epoch [175/200], Train Loss: 0.0083843, VAL--Acc: 93.1301% Se:93.2150 Sp:92.8571 Pr:97.6744 F1: 95.3926% Acc: 93.1301% UAR: 93.0361% \n",
      "Epoch [176/200], Train Loss: 0.0154965, VAL--Acc: 94.3880% Se:95.6880 Sp:90.2041 Pr:96.9171 F1: 96.2987% Acc: 94.3880% UAR: 92.9460% \n",
      "Epoch [177/200], Train Loss: 0.0036365, VAL--Acc: 94.4848% Se:96.9562 Sp:86.5306 Pr:95.8621 F1: 96.4061% Acc: 94.4848% UAR: 91.7434% \n",
      "Epoch [178/200], Train Loss: 0.0039148, VAL--Acc: 92.1142% Se:96.1319 Sp:79.1837 Pr:93.6959 F1: 94.8983% Acc: 92.1142% UAR: 87.6578% \n",
      "Epoch [179/200], Train Loss: 0.0246022, VAL--Acc: 92.5012% Se:92.5174 Sp:92.4490 Pr:97.5267 F1: 94.9561% Acc: 92.5012% UAR: 92.4832% \n",
      "Epoch [180/200], Train Loss: 0.0060814, VAL--Acc: 94.6299% Se:96.9562 Sp:87.1429 Pr:96.0427 F1: 96.4973% Acc: 94.6299% UAR: 92.0496% \n",
      "Epoch [181/200], Train Loss: 0.0006900, VAL--Acc: 94.4848% Se:96.4490 Sp:88.1633 Pr:96.3268 F1: 96.3878% Acc: 94.4848% UAR: 92.3061% \n",
      "Epoch [182/200], Train Loss: 0.0002673, VAL--Acc: 94.4848% Se:97.1465 Sp:85.9184 Pr:95.6902 F1: 96.4128% Acc: 94.4848% UAR: 91.5324% \n",
      "[182] min_loss=0.0002673 \n",
      "Epoch [183/200], Train Loss: 0.0001803, VAL--Acc: 94.8718% Se:96.8294 Sp:88.5714 Pr:96.4624 F1: 96.6456% Acc: 94.8718% UAR: 92.7004% \n",
      "[183] min_loss=0.0001803 \n",
      "Epoch [184/200], Train Loss: 0.0002059, VAL--Acc: 94.6783% Se:96.7660 Sp:87.9592 Pr:96.2776 F1: 96.5212% Acc: 94.6783% UAR: 92.3626% \n",
      "Epoch [185/200], Train Loss: 0.0000861, VAL--Acc: 95.0169% Se:96.7660 Sp:89.3878 Pr:96.7047 F1: 96.7353% Acc: 95.0169% UAR: 93.0769% \n",
      "[185] min_loss=0.0000861 \n",
      "Epoch [186/200], Train Loss: 0.0000495, VAL--Acc: 95.1137% Se:97.2733 Sp:88.1633 Pr:96.3568 F1: 96.8129% Acc: 95.1137% UAR: 92.7183% \n",
      "[186] min_loss=0.0000495 \n",
      "Epoch [187/200], Train Loss: 0.0370701, VAL--Acc: 93.1785% Se:94.2295 Sp:89.7959 Pr:96.7448 F1: 95.4706% Acc: 93.1785% UAR: 92.0127% \n",
      "Epoch [188/200], Train Loss: 0.0182710, VAL--Acc: 94.1461% Se:95.4978 Sp:89.7959 Pr:96.7866 F1: 96.1379% Acc: 94.1461% UAR: 92.6468% \n",
      "Epoch [189/200], Train Loss: 0.0039830, VAL--Acc: 94.0977% Se:97.2099 Sp:84.0816 Pr:95.1583 F1: 96.1731% Acc: 94.0977% UAR: 90.6458% \n",
      "Epoch [190/200], Train Loss: 0.0117344, VAL--Acc: 93.9042% Se:95.8148 Sp:87.7551 Pr:96.1808 F1: 95.9975% Acc: 93.9042% UAR: 91.7850% \n",
      "Epoch [191/200], Train Loss: 0.0100049, VAL--Acc: 94.3396% Se:97.4635 Sp:84.2857 Pr:95.2292 F1: 96.3334% Acc: 94.3396% UAR: 90.8746% \n",
      "Epoch [192/200], Train Loss: 0.0074247, VAL--Acc: 90.5660% Se:88.7127 Sp:96.5306 Pr:98.7994 F1: 93.4848% Acc: 90.5660% UAR: 92.6217% \n",
      "Epoch [193/200], Train Loss: 0.0071771, VAL--Acc: 94.4364% Se:95.8782 Sp:89.7959 Pr:96.7990 F1: 96.3364% Acc: 94.4364% UAR: 92.8371% \n",
      "Epoch [194/200], Train Loss: 0.0068522, VAL--Acc: 93.7107% Se:98.2879 Sp:78.9796 Pr:93.7689 F1: 95.9752% Acc: 93.7107% UAR: 88.6337% \n",
      "Epoch [195/200], Train Loss: 0.0072869, VAL--Acc: 94.8718% Se:96.7026 Sp:88.9796 Pr:96.5801 F1: 96.6413% Acc: 94.8718% UAR: 92.8411% \n",
      "Epoch [196/200], Train Loss: 0.0062927, VAL--Acc: 94.2912% Se:96.7026 Sp:86.5306 Pr:95.8517 F1: 96.2753% Acc: 94.2912% UAR: 91.6166% \n",
      "Epoch [197/200], Train Loss: 0.0063608, VAL--Acc: 93.4688% Se:98.6049 Sp:76.9388 Pr:93.2254 F1: 95.8398% Acc: 93.4688% UAR: 87.7719% \n",
      "Epoch [198/200], Train Loss: 0.0157013, VAL--Acc: 94.7267% Se:96.9562 Sp:87.5510 Pr:96.1635 F1: 96.5583% Acc: 94.7267% UAR: 92.2536% \n",
      "Epoch [199/200], Train Loss: 0.0038306, VAL--Acc: 94.7750% Se:96.1319 Sp:90.4082 Pr:96.9930 F1: 96.5605% Acc: 94.7750% UAR: 93.2700% \n",
      "Training Finished\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T22:38:00.655351Z",
     "start_time": "2025-10-06T22:37:58.457389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = torch.load('model_checkpoint_for_BMD.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "TP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "ori_labels = []\n",
    "pred_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_max = torch.max(labels, 1)\n",
    "        pred_labels.append(predicted)\n",
    "        ori_labels.append(labels_max)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels_max).sum().item()\n",
    "\n",
    "        TN += ((predicted == 1) & (labels_max == 1)).sum().item()\n",
    "        FN += ((predicted == 1) & (labels_max == 0)).sum().item()\n",
    "        TP += ((predicted == 0) & (labels_max == 0)).sum().item()\n",
    "        FP += ((predicted == 0) & (labels_max == 1)).sum().item()\n",
    "\n",
    "Se = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "Sp = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "Pr = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "Acc = (TP + TN) / (TP + FP + TN + FN) if (TP + FP + TN + FN) > 0 else 0\n",
    "UAR = (Se + Sp) / 2\n",
    "F1 = (2 * Pr * Se) / (Pr + Se) if (Pr + Se) > 0 else 0\n",
    "\n",
    "print(f\"TEST-Acc: {(correct_test / total_test) * 100:.4f}% \"\n",
    "      f\"Se:{Se * 100:.4f} \"\n",
    "      f\"Sp:{Sp * 100:.4f} \"\n",
    "      f\"Pr:{Pr * 100:.4f} \"\n",
    "      f\"F1: {F1 * 100:.4f}% \"\n",
    "      f\"Acc: {Acc * 100:.4f}% \"\n",
    "      f\"UAR: {UAR * 100:.4f}% \")"
   ],
   "id": "7adf9542ecf289e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST-Acc: 95.0701% Se:96.0174 Sp:91.7749 Pr:97.5965 F1: 96.8005% Acc: 95.0701% UAR: 93.8962% \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T22:38:04.334242Z",
     "start_time": "2025-10-06T22:38:04.332209Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8f8e32db848efc46",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
